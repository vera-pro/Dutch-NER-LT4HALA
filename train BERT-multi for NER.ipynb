{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel: Python (transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "# import sys  \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install transformers==4.28.0\n",
    "# !{sys.executable} -m pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.28.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvsprovatorova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\r\n"
     ]
    }
   ],
   "source": [
    "! wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"kb-ner\",\n",
    "    \n",
    "# #     # track hyperparameters and run metadata\n",
    "# #     config={\n",
    "# #     \"learning_rate\": 0.02,\n",
    "# #     \"architecture\": \"CNN\",\n",
    "# #     \"dataset\": \"CIFAR-100\",\n",
    "# #     \"epochs\": 10,\n",
    "# #     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/validation.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/transkribus\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/test_VOC.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/train.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/test_NHA.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/test_SA.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/test_RHC.txt\n",
      "/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "for filename in glob.glob('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/*'):\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        content = f.read().decode(errors='replace').strip()\n",
    "        sentences = content.split(\"\\n\\n\")\n",
    "        data = []\n",
    "        for sentence in sentences:\n",
    "            tokens = sentence.split(\"\\n\")\n",
    "            token_data = []\n",
    "            for token in tokens:\n",
    "                token_data.append(token.split())\n",
    "            data.append(token_data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataset(data, label_map):\n",
    "    formatted_data = {\"tokens\": [], \"ner_tags\": []}\n",
    "    for sentence in data:\n",
    "        tokens = [token_data[0] for token_data in sentence if token_data]\n",
    "        ner_tags = [label_map[token_data[1]] for token_data in sentence if token_data]\n",
    "        formatted_data[\"tokens\"].append(tokens)\n",
    "        formatted_data[\"ner_tags\"].append(ner_tags)\n",
    "    return Dataset.from_dict(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = read_conll_file('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/train.txt')\n",
    "val = read_conll_file('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/validation.txt')\n",
    "test_VOC = read_conll_file('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/test_VOC.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(list(set([token_data[1] for sentence in train for token_data in sentence if token_data])))\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-PER': 1,\n",
       " 'B-TIME': 2,\n",
       " 'I-LOC': 3,\n",
       " 'I-PER': 4,\n",
       " 'I-TIME': 5,\n",
       " 'O': 6}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = convert_to_dataset(train, label_map)\n",
    "val_data = convert_to_dataset(val, label_map)\n",
    "test_data = convert_to_dataset(test_VOC, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/ivi/ilps/personal/vprovat/KB/GysBERT\"\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "# model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,num_labels=len(label_list))\n",
    "# model.num_labels = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    predictions, labels = eval_prediction\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"classification_report\": classification_report(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": val_data,\n",
    "    \"test\": test_data,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/91 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/ivi/ilps/personal/vprovat/KB/NER_logs\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
    "\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vprovat/anaconda3/lib/python3.7/site-packages/transformers/optimization.py:395: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvsprovatorova\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vprovat/KB_NER/wandb/run-20230811_112803-gn2qusg6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vsprovatorova/huggingface/runs/gn2qusg6' target=\"_blank\">graceful-resonance-5</a></strong> to <a href='https://wandb.ai/vsprovatorova/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vsprovatorova/huggingface' target=\"_blank\">https://wandb.ai/vsprovatorova/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vsprovatorova/huggingface/runs/gn2qusg6' target=\"_blank\">https://wandb.ai/vsprovatorova/huggingface/runs/gn2qusg6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15075' max='15075' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15075/15075 3:41:43, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>0.116527</td>\n",
       "      <td>0.630705</td>\n",
       "      <td>0.571337</td>\n",
       "      <td>0.599555</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.72      0.61      0.66      6323\n",
       "         PER       0.61      0.56      0.59     11586\n",
       "        TIME       0.56      0.54      0.55      4029\n",
       "\n",
       "   micro avg       0.63      0.57      0.60     21938\n",
       "   macro avg       0.63      0.57      0.60     21938\n",
       "weighted avg       0.63      0.57      0.60     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.108500</td>\n",
       "      <td>0.109948</td>\n",
       "      <td>0.617807</td>\n",
       "      <td>0.625946</td>\n",
       "      <td>0.621850</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.66      0.73      0.69      6323\n",
       "         PER       0.61      0.60      0.60     11586\n",
       "        TIME       0.56      0.55      0.55      4029\n",
       "\n",
       "   micro avg       0.62      0.63      0.62     21938\n",
       "   macro avg       0.61      0.63      0.62     21938\n",
       "weighted avg       0.62      0.63      0.62     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.086500</td>\n",
       "      <td>0.102660</td>\n",
       "      <td>0.643369</td>\n",
       "      <td>0.659176</td>\n",
       "      <td>0.651176</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.67      0.76      0.71      6323\n",
       "         PER       0.64      0.63      0.63     11586\n",
       "        TIME       0.61      0.60      0.60      4029\n",
       "\n",
       "   micro avg       0.64      0.66      0.65     21938\n",
       "   macro avg       0.64      0.66      0.65     21938\n",
       "weighted avg       0.64      0.66      0.65     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.083600</td>\n",
       "      <td>0.086926</td>\n",
       "      <td>0.719388</td>\n",
       "      <td>0.658264</td>\n",
       "      <td>0.687470</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.77      0.73      0.75      6323\n",
       "         PER       0.71      0.64      0.67     11586\n",
       "        TIME       0.66      0.61      0.64      4029\n",
       "\n",
       "   micro avg       0.72      0.66      0.69     21938\n",
       "   macro avg       0.72      0.66      0.69     21938\n",
       "weighted avg       0.72      0.66      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.091851</td>\n",
       "      <td>0.701999</td>\n",
       "      <td>0.683472</td>\n",
       "      <td>0.692611</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.76      0.75      6323\n",
       "         PER       0.69      0.66      0.67     11586\n",
       "        TIME       0.66      0.63      0.65      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.70      0.68      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.089597</td>\n",
       "      <td>0.704582</td>\n",
       "      <td>0.682742</td>\n",
       "      <td>0.693490</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.72      0.76      0.74      6323\n",
       "         PER       0.71      0.66      0.68     11586\n",
       "        TIME       0.67      0.63      0.65      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.70      0.68      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.093565</td>\n",
       "      <td>0.693991</td>\n",
       "      <td>0.693865</td>\n",
       "      <td>0.693928</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.72      0.77      0.74      6323\n",
       "         PER       0.69      0.67      0.68     11586\n",
       "        TIME       0.65      0.64      0.65      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>0.092818</td>\n",
       "      <td>0.703551</td>\n",
       "      <td>0.690081</td>\n",
       "      <td>0.696751</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.77      0.76      6323\n",
       "         PER       0.69      0.67      0.68     11586\n",
       "        TIME       0.66      0.62      0.64      4029\n",
       "\n",
       "   micro avg       0.70      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.70      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.100574</td>\n",
       "      <td>0.704125</td>\n",
       "      <td>0.690902</td>\n",
       "      <td>0.697451</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.73      0.78      0.76      6323\n",
       "         PER       0.70      0.66      0.68     11586\n",
       "        TIME       0.66      0.63      0.64      4029\n",
       "\n",
       "   micro avg       0.70      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.70      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.099842</td>\n",
       "      <td>0.714680</td>\n",
       "      <td>0.685295</td>\n",
       "      <td>0.699679</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.73      0.78      0.75      6323\n",
       "         PER       0.73      0.66      0.69     11586\n",
       "        TIME       0.66      0.62      0.64      4029\n",
       "\n",
       "   micro avg       0.71      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.71      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.109831</td>\n",
       "      <td>0.708831</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.699517</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.77      0.76      6323\n",
       "         PER       0.71      0.66      0.69     11586\n",
       "        TIME       0.64      0.64      0.64      4029\n",
       "\n",
       "   micro avg       0.71      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.71      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.036200</td>\n",
       "      <td>0.101427</td>\n",
       "      <td>0.707728</td>\n",
       "      <td>0.686662</td>\n",
       "      <td>0.697036</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.77      0.76      6323\n",
       "         PER       0.71      0.66      0.68     11586\n",
       "        TIME       0.64      0.64      0.64      4029\n",
       "\n",
       "   micro avg       0.71      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.71      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.028100</td>\n",
       "      <td>0.115169</td>\n",
       "      <td>0.699678</td>\n",
       "      <td>0.684019</td>\n",
       "      <td>0.691760</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.76      0.76      6323\n",
       "         PER       0.69      0.66      0.67     11586\n",
       "        TIME       0.64      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.68      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.109297</td>\n",
       "      <td>0.706289</td>\n",
       "      <td>0.688486</td>\n",
       "      <td>0.697274</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.78      0.76      6323\n",
       "         PER       0.71      0.66      0.68     11586\n",
       "        TIME       0.64      0.62      0.63      4029\n",
       "\n",
       "   micro avg       0.71      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.71      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.119348</td>\n",
       "      <td>0.678932</td>\n",
       "      <td>0.693044</td>\n",
       "      <td>0.685915</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.73      0.77      0.75      6323\n",
       "         PER       0.67      0.67      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.68      0.69      0.69     21938\n",
       "   macro avg       0.68      0.69      0.68     21938\n",
       "weighted avg       0.68      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.123420</td>\n",
       "      <td>0.702709</td>\n",
       "      <td>0.683426</td>\n",
       "      <td>0.692933</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.77      0.76      6323\n",
       "         PER       0.70      0.65      0.68     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.127641</td>\n",
       "      <td>0.698822</td>\n",
       "      <td>0.678913</td>\n",
       "      <td>0.688724</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.77      0.76      0.76      6323\n",
       "         PER       0.69      0.66      0.67     11586\n",
       "        TIME       0.62      0.62      0.62      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.68      0.68     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.123148</td>\n",
       "      <td>0.702988</td>\n",
       "      <td>0.681967</td>\n",
       "      <td>0.692318</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.76      0.77      0.76      6323\n",
       "         PER       0.71      0.66      0.68     11586\n",
       "        TIME       0.61      0.63      0.62      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.68      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.137656</td>\n",
       "      <td>0.700613</td>\n",
       "      <td>0.682378</td>\n",
       "      <td>0.691375</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.76      0.76      6323\n",
       "         PER       0.70      0.66      0.68     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.68      0.69     21938\n",
       "weighted avg       0.70      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.135029</td>\n",
       "      <td>0.689094</td>\n",
       "      <td>0.685796</td>\n",
       "      <td>0.687441</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.78      0.76      6323\n",
       "         PER       0.68      0.66      0.67     11586\n",
       "        TIME       0.62      0.63      0.62      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.68      0.69      0.68     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.143765</td>\n",
       "      <td>0.693854</td>\n",
       "      <td>0.685979</td>\n",
       "      <td>0.689894</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.74      0.79      0.76      6323\n",
       "         PER       0.69      0.65      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.139241</td>\n",
       "      <td>0.707905</td>\n",
       "      <td>0.687802</td>\n",
       "      <td>0.697709</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.76      0.76      0.76      6323\n",
       "         PER       0.70      0.67      0.68     11586\n",
       "        TIME       0.64      0.63      0.64      4029\n",
       "\n",
       "   micro avg       0.71      0.69      0.70     21938\n",
       "   macro avg       0.70      0.69      0.69     21938\n",
       "weighted avg       0.71      0.69      0.70     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.150752</td>\n",
       "      <td>0.694790</td>\n",
       "      <td>0.675312</td>\n",
       "      <td>0.684913</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.76      0.76      0.76      6323\n",
       "         PER       0.68      0.65      0.67     11586\n",
       "        TIME       0.62      0.62      0.62      4029\n",
       "\n",
       "   micro avg       0.69      0.68      0.68     21938\n",
       "   macro avg       0.69      0.68      0.68     21938\n",
       "weighted avg       0.69      0.68      0.68     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.011100</td>\n",
       "      <td>0.147631</td>\n",
       "      <td>0.694101</td>\n",
       "      <td>0.686571</td>\n",
       "      <td>0.690316</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.78      0.76      6323\n",
       "         PER       0.68      0.66      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.155601</td>\n",
       "      <td>0.696542</td>\n",
       "      <td>0.686890</td>\n",
       "      <td>0.691683</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.77      0.76      6323\n",
       "         PER       0.69      0.66      0.67     11586\n",
       "        TIME       0.64      0.64      0.64      4029\n",
       "\n",
       "   micro avg       0.70      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.70      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.693498</td>\n",
       "      <td>0.688440</td>\n",
       "      <td>0.690960</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.78      0.77      6323\n",
       "         PER       0.68      0.66      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.161737</td>\n",
       "      <td>0.694208</td>\n",
       "      <td>0.685158</td>\n",
       "      <td>0.689654</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.78      0.77      6323\n",
       "         PER       0.68      0.65      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.69      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.69      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.162627</td>\n",
       "      <td>0.695330</td>\n",
       "      <td>0.684110</td>\n",
       "      <td>0.689674</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.77      0.76      6323\n",
       "         PER       0.69      0.65      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.70      0.68      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.166187</td>\n",
       "      <td>0.692940</td>\n",
       "      <td>0.684474</td>\n",
       "      <td>0.688681</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.75      0.78      0.77      6323\n",
       "         PER       0.68      0.65      0.66     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.68      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.167166</td>\n",
       "      <td>0.694831</td>\n",
       "      <td>0.684474</td>\n",
       "      <td>0.689614</td>\n",
       "      <td>              precision    recall  f1-score   support\n",
       "\n",
       "         LOC       0.76      0.77      0.77      6323\n",
       "         PER       0.68      0.65      0.67     11586\n",
       "        TIME       0.63      0.63      0.63      4029\n",
       "\n",
       "   micro avg       0.69      0.68      0.69     21938\n",
       "   macro avg       0.69      0.69      0.69     21938\n",
       "weighted avg       0.69      0.68      0.69     21938\n",
       "</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.72      0.61      0.66      6323\n",
      "         PER       0.61      0.56      0.59     11586\n",
      "        TIME       0.56      0.54      0.55      4029\n",
      "\n",
      "   micro avg       0.63      0.57      0.60     21938\n",
      "   macro avg       0.63      0.57      0.60     21938\n",
      "weighted avg       0.63      0.57      0.60     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.66      0.73      0.69      6323\n",
      "         PER       0.61      0.60      0.60     11586\n",
      "        TIME       0.56      0.55      0.55      4029\n",
      "\n",
      "   micro avg       0.62      0.63      0.62     21938\n",
      "   macro avg       0.61      0.63      0.62     21938\n",
      "weighted avg       0.62      0.63      0.62     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.67      0.76      0.71      6323\n",
      "         PER       0.64      0.63      0.63     11586\n",
      "        TIME       0.61      0.60      0.60      4029\n",
      "\n",
      "   micro avg       0.64      0.66      0.65     21938\n",
      "   macro avg       0.64      0.66      0.65     21938\n",
      "weighted avg       0.64      0.66      0.65     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.77      0.73      0.75      6323\n",
      "         PER       0.71      0.64      0.67     11586\n",
      "        TIME       0.66      0.61      0.64      4029\n",
      "\n",
      "   micro avg       0.72      0.66      0.69     21938\n",
      "   macro avg       0.72      0.66      0.69     21938\n",
      "weighted avg       0.72      0.66      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.76      0.75      6323\n",
      "         PER       0.69      0.66      0.67     11586\n",
      "        TIME       0.66      0.63      0.65      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.70      0.68      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.72      0.76      0.74      6323\n",
      "         PER       0.71      0.66      0.68     11586\n",
      "        TIME       0.67      0.63      0.65      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.70      0.68      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.72      0.77      0.74      6323\n",
      "         PER       0.69      0.67      0.68     11586\n",
      "        TIME       0.65      0.64      0.65      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.77      0.76      6323\n",
      "         PER       0.69      0.67      0.68     11586\n",
      "        TIME       0.66      0.62      0.64      4029\n",
      "\n",
      "   micro avg       0.70      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.70      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.73      0.78      0.76      6323\n",
      "         PER       0.70      0.66      0.68     11586\n",
      "        TIME       0.66      0.63      0.64      4029\n",
      "\n",
      "   micro avg       0.70      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.70      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.73      0.78      0.75      6323\n",
      "         PER       0.73      0.66      0.69     11586\n",
      "        TIME       0.66      0.62      0.64      4029\n",
      "\n",
      "   micro avg       0.71      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.71      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.77      0.76      6323\n",
      "         PER       0.71      0.66      0.69     11586\n",
      "        TIME       0.64      0.64      0.64      4029\n",
      "\n",
      "   micro avg       0.71      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.71      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.77      0.76      6323\n",
      "         PER       0.71      0.66      0.68     11586\n",
      "        TIME       0.64      0.64      0.64      4029\n",
      "\n",
      "   micro avg       0.71      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.71      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.76      0.76      6323\n",
      "         PER       0.69      0.66      0.67     11586\n",
      "        TIME       0.64      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.68      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.78      0.76      6323\n",
      "         PER       0.71      0.66      0.68     11586\n",
      "        TIME       0.64      0.62      0.63      4029\n",
      "\n",
      "   micro avg       0.71      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.71      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.73      0.77      0.75      6323\n",
      "         PER       0.67      0.67      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.68      0.69      0.69     21938\n",
      "   macro avg       0.68      0.69      0.68     21938\n",
      "weighted avg       0.68      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.77      0.76      6323\n",
      "         PER       0.70      0.65      0.68     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.77      0.76      0.76      6323\n",
      "         PER       0.69      0.66      0.67     11586\n",
      "        TIME       0.62      0.62      0.62      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.68      0.68     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.76      0.77      0.76      6323\n",
      "         PER       0.71      0.66      0.68     11586\n",
      "        TIME       0.61      0.63      0.62      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.68      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.76      0.76      6323\n",
      "         PER       0.70      0.66      0.68     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.68      0.69     21938\n",
      "weighted avg       0.70      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.78      0.76      6323\n",
      "         PER       0.68      0.66      0.67     11586\n",
      "        TIME       0.62      0.63      0.62      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.68      0.69      0.68     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.74      0.79      0.76      6323\n",
      "         PER       0.69      0.65      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.76      0.76      0.76      6323\n",
      "         PER       0.70      0.67      0.68     11586\n",
      "        TIME       0.64      0.63      0.64      4029\n",
      "\n",
      "   micro avg       0.71      0.69      0.70     21938\n",
      "   macro avg       0.70      0.69      0.69     21938\n",
      "weighted avg       0.71      0.69      0.70     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.76      0.76      0.76      6323\n",
      "         PER       0.68      0.65      0.67     11586\n",
      "        TIME       0.62      0.62      0.62      4029\n",
      "\n",
      "   micro avg       0.69      0.68      0.68     21938\n",
      "   macro avg       0.69      0.68      0.68     21938\n",
      "weighted avg       0.69      0.68      0.68     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.78      0.76      6323\n",
      "         PER       0.68      0.66      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.77      0.76      6323\n",
      "         PER       0.69      0.66      0.67     11586\n",
      "        TIME       0.64      0.64      0.64      4029\n",
      "\n",
      "   micro avg       0.70      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.70      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.78      0.77      6323\n",
      "         PER       0.68      0.66      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.78      0.77      6323\n",
      "         PER       0.68      0.65      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.69      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.69      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.77      0.76      6323\n",
      "         PER       0.69      0.65      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.70      0.68      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.75      0.78      0.77      6323\n",
      "         PER       0.68      0.65      0.66     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.68      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.76      0.77      0.77      6323\n",
      "         PER       0.68      0.65      0.67     11586\n",
      "        TIME       0.63      0.63      0.63      4029\n",
      "\n",
      "   micro avg       0.69      0.68      0.69     21938\n",
      "   macro avg       0.69      0.69      0.69     21938\n",
      "weighted avg       0.69      0.68      0.69     21938\n",
      "\" of type <class 'str'> for key \"eval/classification_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15075, training_loss=0.040053038440928924, metrics={'train_runtime': 13317.2331, 'train_samples_per_second': 9.056, 'train_steps_per_second': 1.132, 'total_flos': 3.15138133702656e+16, 'train_loss': 0.040053038440928924, 'epoch': 15.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.config.id2label = id2label\n",
    "trainer.model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-LOC',\n",
       " 1: 'B-PER',\n",
       " 2: 'B-TIME',\n",
       " 3: 'I-LOC',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-TIME',\n",
       " 6: 'O'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 96074, 10145, 85174, 12796, 18999, 10106, 10235, 30724,   117,\n",
      "         10110, 47458, 12796, 10263, 10106, 29485, 31417,   102]],\n",
      "       device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')}\n",
      "tensor([6, 6, 6, 4, 6, 6, 6, 6, 0, 6, 6, 6, 6, 6, 6, 6, 6, 6], device='cuda:0')\n",
      "Named Entities - Example 1: [('[CLS]', 'O'), ('Wilhelmina', 'O'), ('van', 'O'), ('Oosten', 'I-PER'), ('wo', 'O'), ('##ont', 'O'), ('in', 'O'), ('Den', 'O'), ('Haag', 'B-LOC'), (',', 'O'), ('en', 'O'), ('ik', 'O'), ('wo', 'O'), ('##on', 'O'), ('in', 'O'), ('Bus', 'O'), ('##sum', 'O'), ('[SEP]', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Wilhelmina van Oosten woont in Den Haag, en ik woon in Bussum\"\n",
    "\n",
    "\n",
    "tokenized_input = tokenizer(sentence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(tokenized_input)\n",
    "outputs = model(**tokenized_input)\n",
    "\n",
    "# print(outputs)\n",
    "# print( outputs.logits.argmax(-1))\n",
    "predicted_labels = outputs.logits.argmax(-1)[0]\n",
    "print(predicted_labels)\n",
    "\n",
    "\n",
    "named_entities = [(tokenizer.decode([token]), \n",
    "                   id2label[int(label)]\n",
    "                  ) \n",
    "                  for token, label in zip(tokenized_input[\"input_ids\"][0], predicted_labels)]\n",
    "\n",
    "\n",
    "print(\"Named Entities - Example 1:\", named_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8040"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_auto_class',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_create_repo',\n",
       " '_expand_inputs_for_generation',\n",
       " '_extract_past_from_model_output',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_from_config',\n",
       " '_get_backward_hooks',\n",
       " '_get_decoder_start_token_id',\n",
       " '_get_files_timestamps',\n",
       " '_get_logits_processor',\n",
       " '_get_logits_warper',\n",
       " '_get_name',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_stopping_criteria',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_weights',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_state_dict',\n",
       " '_load_pretrained_model',\n",
       " '_load_pretrained_model_low_mem',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_model_inputs',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " 'add_memory_hooks',\n",
       " 'add_module',\n",
       " 'adjust_logits_during_generation',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'beam_sample',\n",
       " 'beam_search',\n",
       " 'bert',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'constrained_beam_search',\n",
       " 'contrastive_search',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'device',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generation_config',\n",
       " 'get_buffer',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'greedy_search',\n",
       " 'group_beam_search',\n",
       " 'half',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_loaded_in_8bit',\n",
       " 'is_parallelizable',\n",
       " 'load_state_dict',\n",
       " 'load_tf_weights',\n",
       " 'main_input_name',\n",
       " 'modules',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_labels',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'sample',\n",
       " 'save_pretrained',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_sm_patterns_to_gitignore',\n",
       " '_gather_and_numpify',\n",
       " '_get_collator_with_removed_columns',\n",
       " '_get_eval_sampler',\n",
       " '_get_learning_rate',\n",
       " '_get_output_dir',\n",
       " '_get_train_sampler',\n",
       " '_globalstep_last_logged',\n",
       " '_hp_search_setup',\n",
       " '_inner_training_loop',\n",
       " '_issue_warnings_after_load',\n",
       " '_load_best_model',\n",
       " '_load_from_checkpoint',\n",
       " '_load_optimizer_and_scheduler',\n",
       " '_load_rng_state',\n",
       " '_loggers_initialized',\n",
       " '_maybe_log_save_evaluate',\n",
       " '_memory_tracker',\n",
       " '_move_model_to_device',\n",
       " '_nested_gather',\n",
       " '_pad_across_processes',\n",
       " '_prepare_input',\n",
       " '_prepare_inputs',\n",
       " '_push_from_checkpoint',\n",
       " '_remove_unused_columns',\n",
       " '_report_to_hp_search',\n",
       " '_rotate_checkpoints',\n",
       " '_save',\n",
       " '_save_checkpoint',\n",
       " '_save_tpu',\n",
       " '_set_signature_columns_if_needed',\n",
       " '_signature_columns',\n",
       " '_sorted_checkpoints',\n",
       " '_total_loss_scalar',\n",
       " '_train_batch_size',\n",
       " '_trial',\n",
       " '_tune_save_checkpoint',\n",
       " '_wrap_model',\n",
       " 'add_callback',\n",
       " 'args',\n",
       " 'autocast_smart_context_manager',\n",
       " 'call_model_init',\n",
       " 'callback_handler',\n",
       " 'can_return_loss',\n",
       " 'compute_loss',\n",
       " 'compute_loss_context_manager',\n",
       " 'compute_metrics',\n",
       " 'control',\n",
       " 'create_model_card',\n",
       " 'create_optimizer',\n",
       " 'create_optimizer_and_scheduler',\n",
       " 'create_scheduler',\n",
       " 'current_flos',\n",
       " 'data_collator',\n",
       " 'deepspeed',\n",
       " 'do_grad_scaling',\n",
       " 'eval_dataset',\n",
       " 'evaluate',\n",
       " 'evaluation_loop',\n",
       " 'floating_point_ops',\n",
       " 'fsdp',\n",
       " 'get_eval_dataloader',\n",
       " 'get_optimizer_cls_and_kwargs',\n",
       " 'get_test_dataloader',\n",
       " 'get_train_dataloader',\n",
       " 'hp_name',\n",
       " 'hp_search_backend',\n",
       " 'hyperparameter_search',\n",
       " 'init_git_repo',\n",
       " 'ipex_optimize_model',\n",
       " 'is_in_train',\n",
       " 'is_local_process_zero',\n",
       " 'is_model_parallel',\n",
       " 'is_world_process_zero',\n",
       " 'label_names',\n",
       " 'label_smoother',\n",
       " 'log',\n",
       " 'log_metrics',\n",
       " 'lr_scheduler',\n",
       " 'metrics_format',\n",
       " 'model',\n",
       " 'model_init',\n",
       " 'model_wrapped',\n",
       " 'num_examples',\n",
       " 'optimizer',\n",
       " 'place_model_on_device',\n",
       " 'pop_callback',\n",
       " 'predict',\n",
       " 'prediction_loop',\n",
       " 'prediction_step',\n",
       " 'preprocess_logits_for_metrics',\n",
       " 'push_to_hub',\n",
       " 'remove_callback',\n",
       " 'save_metrics',\n",
       " 'save_model',\n",
       " 'save_state',\n",
       " 'sharded_ddp',\n",
       " 'state',\n",
       " 'store_flos',\n",
       " 'tokenizer',\n",
       " 'torch_jit_model_eval',\n",
       " 'train',\n",
       " 'train_dataset',\n",
       " 'training_step',\n",
       " 'use_apex',\n",
       " 'use_cpu_amp',\n",
       " 'use_cuda_amp',\n",
       " 'use_tune_checkpoints']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label_map = id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-LOC',\n",
       " 1: 'B-PER',\n",
       " 2: 'B-TIME',\n",
       " 3: 'I-LOC',\n",
       " 4: 'I-PER',\n",
       " 5: 'I-TIME',\n",
       " 6: 'O'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('/ivi/ilps/personal/vprovat/KB/models/BERT-multi-NER-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-77ded6970c32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/ivi/ilps/personal/vprovat/KB/models/BERT-multi-NER-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/ivi/ilps/personal/vprovat/KB/models/BERT-multi-NER-v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/ivi/ilps/personal/vprovat/KB/models/BERT-multi-NER-v1\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/ivi/ilps/personal/vprovat/KB/models/BERT-multi-NER-v1\")\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"Ik ben Maurits van Bussum en ik woon in Zandvoort aan zee sinds 1987\"\n",
    "ner_results = nlp(example)\n",
    "\n",
    "res =[post_process(item) for item in ner_results]\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabi",
   "language": "python",
   "name": "tabi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
