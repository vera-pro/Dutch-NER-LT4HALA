{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_old = 'test_NHA.txt  test_RHC.txt  test_SA.txt  test_VOC.txt'.split()\n",
    "# tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = tests_old +['ned.testb'] # for the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54bb62a9712b4f25900bb60b92cf3403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf068683f3043e6b83de0dd0afce10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42ef60f0b61497ba29e0497baf943bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c8610b05464de18d4323374dbbca8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2742545eee614d1b8a5b0548c6f1d660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4705444cc37c4372bad9d759e5ebc9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae41984cc7f6423ea175873ecb66eba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, AutoModel\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from data_utils import prepare_data, convert_to_dataset\n",
    "\n",
    "# train and val are redundant but we need the labels, sooo\n",
    "train = prepare_data('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/train.txt')\n",
    "val = prepare_data('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/validation.txt')\n",
    "\n",
    "tests_prepared = [\n",
    "     prepare_data('/ivi/ilps/personal/vprovat/KB/data/AITrainingset/Data/'+test) for test in tests_old\n",
    "]\n",
    "\n",
    "tests_prepared += [\n",
    "     prepare_data('/ivi/ilps/personal/vprovat/KB/data/Dutch_conll/ned.testb')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(list(set([token_data[1] for sentence in train for token_data in sentence if token_data])))\n",
    "\n",
    "label_list = label_list\n",
    "# print(label_list_new)\n",
    "id2label_original = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "label_map = {label: i for i, label in enumerate(label_list)}\n",
    "label_map.update({'B-ORG': label2id['O'],\n",
    "                  'B-MISC': label2id['O'], \n",
    "                  'I-ORG': label2id['O'],\n",
    "                  'I-MISC': label2id['O']})\n",
    "\n",
    "train_data = convert_to_dataset(train, label_map)\n",
    "val_data = convert_to_dataset(val, label_map)\n",
    "test_data = [convert_to_dataset(test, label_map)\n",
    "             for test in tests_prepared]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-PER', 'B-TIME', 'I-LOC', 'I-PER', 'I-TIME', 'O']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = id2label_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\n",
    "    \"train\": train_data,\n",
    "    \"validation\": val_data}\n",
    "for i, test in enumerate(tests):\n",
    "    dct[test.split('.')[0]] = test_data[i]\n",
    "\n",
    "datasets = DatasetDict(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model_name, dataset_name):\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=512)\n",
    "    global model\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "    tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)\n",
    "    \n",
    "    global id2label\n",
    "    id2label = model.config.id2label\n",
    "#     global label2id\n",
    "#     label2id = model.config.label2id\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "\n",
    "    preds = trainer.predict(tokenized_datasets[dataset_name])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_predictions(preds):\n",
    "    predictions = np.argmax(preds.predictions, axis=2)\n",
    "    labels = preds.label_ids\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label_original[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return true_predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wrong_predictions(preds): # experimental, will be improved\n",
    "    true_predictions, true_labels = clean_predictions(preds)\n",
    "    \n",
    "    res = []\n",
    "    for i in range(len(true_predictions)):\n",
    "        if true_predictions[i] != true_labels[i]:\n",
    "            res.append((i, true_predictions[i],true_labels[i]))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_prediction):\n",
    "    true_predictions, true_labels = clean_predictions(eval_prediction)\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"classification_report\": classification_report(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True, padding=True, return_tensors='pt'\n",
    "    )\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def data_collator(data):\n",
    "    input_ids = [torch.tensor(item[\"input_ids\"]) for item in data]\n",
    "    attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in data]\n",
    "    labels = [torch.tensor(item[\"labels\"]) for item in data]\n",
    "\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {'GysBERT': \"/ivi/ilps/personal/vprovat/KB/models/GysBERT-NER-v2\",\n",
    "              'BERTje': \"/ivi/ilps/personal/vprovat/KB/models/BERTje-NER-v2\",\n",
    "              'BERT-multi-cased': \"/ivi/ilps/personal/vprovat/KB/models/BERT-multi-cased-NER-v2\",\n",
    "              'WikiNEuRal': \"Babelscape/wikineural-multilingual-ner\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "all_preds=pickle.load(open('all_predictions.p','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wrong_preds = {}\n",
    "\n",
    "for model_name in model_names.keys():\n",
    "    wrong_preds = {}\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_names[model_name]) # just to get the labels for WikiNEuRal\n",
    "    id2label = model.config.id2label\n",
    "    for test in tests:\n",
    "        preds = all_preds[model_name][test]\n",
    "        wrong_preds[test] = extract_wrong_predictions(preds)\n",
    "    all_wrong_preds[model_name] = wrong_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(all_wrong_preds, open('all_wrong_preds.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_NHA.txt', 'test_RHC.txt', 'test_SA.txt', 'test_VOC.txt', 'ned.testb']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabi",
   "language": "python",
   "name": "tabi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
