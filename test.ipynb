{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Mention:\n",
    "    \"\"\"Entity mention and its context\"\"\"\n",
    "    mention: str\n",
    "    doc_title: str\n",
    "    left_context: str\n",
    "    right_context: str\n",
    "        \n",
    "    def prepare_for_tokenizer(self):\n",
    "#         return f\"[CLS]{self.doc_title}[SEP]{self.left_context}[E]{self.mention}[/E]{self.right_context}[SEP]\"\n",
    "        return f\"{self.doc_title}[SEP]{self.left_context}[E]{self.mention}[/E]{self.right_context}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Entity:\n",
    "    \"\"\"Entity and its description. For the future - maybe add synonyms and neighbours\"\"\"\n",
    "#     entity: str\n",
    "    description: str\n",
    "#     qid: str\n",
    "        \n",
    "    def prepare_for_tokenizer(self):\n",
    "        return f\"{self.description}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BertTokenizer.from_pretrained(\"emanjavacas/GysBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emanjavacas/GysBERT were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"emanjavacas/GysBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30002, 768)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens_dict = {'additional_special_tokens': ['[E]','[/E]']}\n",
    "num_added_toks = tok.add_special_tokens(special_tokens_dict)\n",
    "model.resize_token_embeddings(len(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ment = Mention(mention='Amsterdam', doc_title='Aan de Amsterdamse grachten',\n",
    "              left_context='Er staat een huis aan de gracht in oud',\n",
    "              right_context='waar ik als jochie van acht bij grootmoeder kwam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = \"stad in Noord-Holland, Nederland\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_entry(entry):\n",
    "    '''\n",
    "    entry is either mention (with context) or entity (with description)\n",
    "    '''\n",
    "    input_line = mention.prepare_for_tokenizer()\n",
    "    return tok(input_line,return_tensors='pt')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  1490,  1448,  1945,  1803, 23511,     3,  1557,  2010,  1473,\n",
       "          2026,  1490,  1448, 12050,  1464,  2602, 30000,  1945, 30001,  1562,\n",
       "          1642,  1560,  4408, 20065,   905,  1455,  2408,  1534, 19927,  2601,\n",
       "             3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize_mention(ment)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30002, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.6718,  0.2235, -0.2448,  ..., -0.5155,  0.3100,  0.4625],\n",
       "         [ 0.0170, -1.0129, -0.6136,  ..., -0.7572,  0.8658, -0.0089],\n",
       "         [-0.2262, -0.9541, -0.8106,  ..., -0.3831,  0.6160,  0.4601],\n",
       "         ...,\n",
       "         [ 0.4031,  0.5679, -0.7611,  ..., -0.4276, -0.0226,  1.6612],\n",
       "         [ 0.5952, -0.2189, -0.3749,  ..., -0.8822,  0.8337,  0.3204],\n",
       "         [ 0.4837, -0.2339, -0.6935,  ..., -0.4878,  0.6203,  0.1989]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.1874,  0.0314, -0.0723, -0.9831, -0.1143,  1.0000,  0.1045,  0.0174,\n",
       "          0.1398, -0.0815, -0.2276, -0.4845, -0.8867,  0.9994, -0.0050, -0.6438,\n",
       "         -0.5974, -0.4487, -0.1266,  0.2979,  0.5769,  0.0676, -0.0367,  0.8939,\n",
       "          0.5296,  0.0498, -0.0557,  0.5008,  0.6975, -0.0699, -1.0000,  0.9631,\n",
       "          1.0000,  0.1897, -0.2364, -0.9996, -1.0000,  0.0640, -0.1700,  0.9998,\n",
       "          0.0656, -0.1328,  0.9231,  0.0598, -0.2563, -0.9992, -0.9999,  0.0366,\n",
       "         -0.1571,  0.1380,  0.9855,  0.1759,  0.0360,  0.0278, -1.0000,  0.2149,\n",
       "         -0.1226,  0.0972,  0.0970, -0.0878, -0.3387, -0.0631,  0.0814,  0.0390,\n",
       "         -0.1706, -0.8364,  0.1563, -0.9998,  0.2333,  0.6363, -0.9993,  0.1916,\n",
       "         -0.1165,  0.0962, -0.8371, -0.1484,  0.9014,  0.8214,  0.3896,  0.2783,\n",
       "         -0.1570,  0.3593,  0.1824,  0.0935,  0.2891, -0.3595, -0.0640,  0.4491,\n",
       "         -0.1570,  0.9999, -0.9604,  0.1628,  0.9982,  0.1883, -0.0748,  0.1021,\n",
       "         -1.0000, -1.0000, -0.2686,  0.0387, -0.2108, -0.2209, -0.0813, -0.3332,\n",
       "          0.2699,  0.0674, -0.2655, -0.0532,  0.0400, -0.1086,  0.0254, -0.3443,\n",
       "          0.4043, -0.0858, -1.0000, -0.1740,  0.1020, -0.7687,  0.9978,  0.0946,\n",
       "          0.0687,  0.9772, -0.1371,  0.9923,  0.9935,  0.1195,  0.1567, -0.5211,\n",
       "         -0.2656,  0.0929, -0.1895, -0.9895, -1.0000,  0.0986,  0.2583,  0.9342,\n",
       "          0.0101,  0.3078, -0.1194,  0.9999, -0.0842,  0.1431, -0.3274, -0.2352,\n",
       "          0.9716,  0.0926, -0.0594,  0.3766, -0.0323, -0.0379, -0.9976,  0.2069,\n",
       "         -0.6664,  0.8495,  0.8590, -0.9983, -0.3348,  0.9613,  0.8230,  0.1208,\n",
       "          0.2633, -0.7824, -0.9767,  0.3699, -0.9996, -0.4889, -0.9795, -0.9999,\n",
       "         -0.9776, -0.0616,  0.2688,  0.9488, -0.1880,  0.3507,  0.9671, -0.0244,\n",
       "         -0.0474, -0.2330,  1.0000,  0.8522,  0.2302,  0.3016, -0.6816, -0.9586,\n",
       "         -0.0754,  0.9436,  0.0852, -0.0954, -0.0697, -0.2484, -0.1853,  0.0513,\n",
       "         -0.1356, -0.9473, -1.0000,  0.3353,  0.9888,  0.0334, -0.2854,  0.0138,\n",
       "          0.1239, -0.1468, -1.0000, -0.1565, -0.9779, -0.9968, -0.7930, -0.1009,\n",
       "          0.1519, -0.0090,  0.8217,  0.2758, -0.2456,  0.1068,  0.9829, -0.4036,\n",
       "          0.9897, -0.8807, -0.0693, -1.0000,  0.6586, -0.2246, -0.8788, -0.6145,\n",
       "         -0.9906,  0.9881,  0.0620,  0.2823,  0.1514,  0.9974, -1.0000, -0.0335,\n",
       "         -0.9994,  0.2047, -0.3596,  0.9477,  0.9996,  0.7107,  1.0000, -1.0000,\n",
       "          0.1548,  0.9672,  0.2181,  0.1340,  0.9607, -0.4021,  0.0734, -0.1429,\n",
       "         -0.2560,  0.1901,  0.2710,  0.3676, -0.6718, -0.3201,  0.0678,  0.4344,\n",
       "         -0.0659, -0.1223, -0.1455, -0.1200,  0.9968,  1.0000, -0.2068,  0.6289,\n",
       "         -0.2228,  0.2423,  0.2930, -0.9115, -0.1409, -0.6009, -0.0278,  0.2204,\n",
       "         -0.9992,  0.0122,  0.9902, -0.0236,  0.1078, -0.2047, -0.0637, -0.5675,\n",
       "          0.2921,  0.9984, -0.3383,  0.2416, -0.4374,  0.2423, -0.9951, -0.8682,\n",
       "          0.0431, -1.0000,  0.0660,  0.0138,  0.0542, -0.1347,  0.2064, -0.0352,\n",
       "         -0.0060, -0.3154,  0.0838,  0.0699,  0.9995,  0.2697, -0.2148, -0.2396,\n",
       "         -0.5917,  0.5779, -0.9963,  0.9998, -0.8049, -0.9961,  0.0743,  0.1154,\n",
       "         -0.1576,  0.9993, -0.5437, -0.4162, -0.1873, -0.0149,  0.2091,  0.4117,\n",
       "         -0.0354,  0.1184,  0.6276, -0.8638,  0.1101,  0.1488, -0.7845,  0.9958,\n",
       "          0.1871, -1.0000,  0.1580, -0.9177,  0.1663, -0.9697,  0.0949,  0.1304,\n",
       "         -0.9999,  0.3392,  0.1797, -0.9464,  0.9774,  0.1714,  0.2718,  0.0199,\n",
       "          0.9995,  0.1303,  0.1243, -0.0776,  0.9983, -0.0076, -0.1518,  0.0422,\n",
       "          0.5874, -0.0674, -0.1196, -0.1624, -0.9997, -0.3554,  0.0789, -0.1717,\n",
       "         -0.1590, -0.2181, -0.0821, -0.0163, -0.3885,  0.1360, -0.3516, -0.9652,\n",
       "         -0.1206, -0.9430,  0.4238,  0.0886,  1.0000,  0.0059, -0.9995,  0.3537,\n",
       "         -0.9845,  0.4829,  0.8950,  0.0399, -0.9865,  0.0430,  0.1533,  0.0165,\n",
       "         -0.1848, -0.9964,  0.1729,  0.2152,  0.9955,  0.0291, -0.0282, -0.0984,\n",
       "         -0.9989, -0.2287,  0.7550, -0.0777, -0.1318, -0.1398, -0.9993, -0.9997,\n",
       "          0.0325,  1.0000, -0.4397, -0.0488, -0.1165, -0.0786,  0.4237, -0.6762,\n",
       "          0.0301, -0.0199, -0.7563,  0.9977,  0.2430, -0.2070, -0.4567, -0.1916,\n",
       "          0.0427,  0.1262,  0.7869,  0.9969,  0.3093,  0.0587,  0.3712,  0.4019,\n",
       "         -0.0303,  0.1899, -0.1603, -0.9830, -0.1756, -0.1219, -0.2375, -1.0000,\n",
       "         -0.9986, -0.9260, -0.0573,  0.1094, -0.8878, -0.4064, -0.9961,  0.9888,\n",
       "          0.0746, -0.5079,  0.0523,  0.3708,  0.1957,  1.0000,  0.3486,  1.0000,\n",
       "          0.2142, -0.0267, -0.9971, -0.1230, -0.9616, -0.2522, -0.6704,  0.9275,\n",
       "          0.9764, -0.1633,  0.2150, -0.0993, -0.0674, -0.9996, -0.0984, -0.2454,\n",
       "         -0.1372, -0.0976,  0.9997,  0.3534, -0.0406,  0.1116,  0.2750, -0.0595,\n",
       "          0.1544,  0.9821, -0.0369, -0.3028, -0.9994,  0.6305, -1.0000, -1.0000,\n",
       "         -0.7790,  0.0102, -1.0000, -0.0228,  0.4808, -0.1785,  0.3356,  0.3006,\n",
       "          0.1194,  0.0546, -0.1152, -0.9999, -0.1861,  0.9976, -0.1261,  0.0791,\n",
       "          0.9044,  0.1965,  0.5512, -0.2981,  0.2058,  0.1393, -0.9715, -0.9913,\n",
       "         -0.2366,  0.0662,  0.2067,  0.0874,  0.1177,  0.1836, -0.1693, -0.8194,\n",
       "         -0.9995,  0.3265,  0.1971, -0.0462,  0.0688,  0.9477, -0.3995, -0.2156,\n",
       "          0.0400, -0.8912, -0.2799, -0.9920,  0.2775,  0.9981,  0.3443,  0.0917,\n",
       "         -0.9309,  0.1547,  0.1063,  0.0298, -0.5748, -0.9790, -0.9687, -1.0000,\n",
       "         -0.9967,  0.2056, -0.1400,  0.7205,  0.9997,  0.1339, -0.0295,  0.2251,\n",
       "          0.2413,  0.1804,  0.1770,  0.2284,  0.3081, -0.0612, -0.1515, -0.9993,\n",
       "         -0.9999,  0.9996,  0.2289,  0.3707, -0.3952, -0.2261, -0.2879, -0.9558,\n",
       "          0.0656,  0.1388,  0.3622,  0.0853,  1.0000,  0.1498, -0.2080, -0.3797,\n",
       "          0.9956, -0.0322,  0.5827, -0.9124, -0.9174, -0.5985, -0.0953, -1.0000,\n",
       "         -0.9994,  0.4030,  0.2993,  0.2576,  0.9932, -1.0000, -0.2286, -1.0000,\n",
       "         -0.4119, -0.9629, -0.1620, -0.1454, -0.1713,  0.1701, -0.2584,  0.1838,\n",
       "         -0.9998,  0.6117, -0.9970, -0.1280,  0.9958,  0.3352,  0.2396, -0.4345,\n",
       "          0.3062,  0.3463,  0.9867,  0.9999, -0.0781,  0.2138,  0.1045, -0.9328,\n",
       "         -0.2393,  0.0903,  0.1144,  0.0426, -0.0643, -0.2651, -0.2666, -1.0000,\n",
       "          0.6242,  0.9999, -0.3514, -0.1908,  1.0000,  0.0551, -0.0124,  1.0000,\n",
       "         -0.4939, -0.0154,  0.2240, -0.0116, -0.6826,  0.1328, -1.0000, -0.1928,\n",
       "          0.9990, -0.1719,  0.1039, -0.9852, -0.1210, -0.1780, -0.0154, -0.0092,\n",
       "          0.1296,  0.1199, -0.0409,  0.2440,  0.9310, -0.1010,  0.2714, -0.0452,\n",
       "         -0.0903, -1.0000, -0.0424,  0.0349,  0.3203,  0.0592, -0.1661, -1.0000,\n",
       "          0.0713, -0.9993, -0.1196,  0.1288, -0.0338,  0.9054,  0.9941, -1.0000,\n",
       "         -0.9839, -0.2634,  0.1968, -0.0511,  0.1128,  0.3414, -0.2866,  0.1809,\n",
       "          0.2598, -0.1419,  0.1957,  0.0743, -0.0748, -0.6762,  0.2220,  0.0619,\n",
       "          0.0883,  0.5409, -0.9687,  0.0894, -0.0997,  0.0746, -0.0149,  0.3132,\n",
       "         -0.0742, -0.0648, -0.0588, -0.2729, -0.7837,  0.6326, -0.0307, -0.0297,\n",
       "         -0.5025,  0.9994,  0.0734,  1.0000, -0.7556, -0.9731,  0.0538,  0.7069,\n",
       "         -0.2129, -0.9612,  0.3905, -0.1376, -0.1170,  0.3274,  0.0851, -0.0457,\n",
       "          0.2513,  0.7275, -0.1523, -0.1006, -0.0175, -0.0770,  0.9949,  1.0000,\n",
       "         -0.3399, -0.1267, -0.1842,  0.2921, -0.1111,  0.0595, -1.0000, -0.0208,\n",
       "          0.0876,  0.9989, -0.1645,  0.0713,  0.9996,  1.0000, -0.3539, -0.0260,\n",
       "         -0.0388, -0.7845,  1.0000, -0.2129,  0.0628,  0.9998, -0.0131,  0.3471,\n",
       "         -0.3751,  0.0271,  0.2994, -0.2008, -0.9945, -0.2002,  0.3981, -0.9999,\n",
       "          0.0805,  0.9997,  0.0481,  1.0000, -0.0638,  0.9375, -0.0551, -0.9999,\n",
       "         -1.0000, -0.0026, -0.1911,  0.0401,  0.9993,  0.1222, -0.9937, -0.9896]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model(tokens)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nel",
   "language": "python",
   "name": "nel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
